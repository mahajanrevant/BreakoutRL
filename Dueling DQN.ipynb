{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import Environment\n",
    "from agent import Agent\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from agent import Agent\n",
    "#from dqn_model import DQN\n",
    "\n",
    "torch.manual_seed(595)\n",
    "np.random.seed(595)\n",
    "random.seed(595)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 3 convolutional layers followed by\n",
    "# 2 fully-connected layers. The first convolutional layer has\n",
    "# 32 8×8 filters with stride 4, the second 64 4×4 filters with\n",
    "# stride 2, and the third and final convolutional layer consists\n",
    "# 64 3 × 3 filters with stride 1. As shown in Figure 1, the\n",
    "# dueling network splits into two streams of fully connected\n",
    "# layers. The value and advantage streams both have a fullyconnected layer with 512 units. The final hidden layers of\n",
    "# the value and advantage streams are both fully-connected\n",
    "# with the value stream having one output and the advantage\n",
    "# as many outputs as there are valid actions2\n",
    "# . We combine the\n",
    "# value and advantage streams using the module described by\n",
    "# Equation (9). Rectifier non-linearities (Fukushima, 1980)\n",
    "# are inserted between all adjacent layers.\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    \"\"\"Initialize a deep Q-learning network\n",
    "\n",
    "    Hints:\n",
    "    -----\n",
    "        Original paper for DQN\n",
    "    https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "\n",
    "    This is just a hint. You can build your own structure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=4, num_actions=4):\n",
    "        \n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        in_channels: number of channel of input.\n",
    "                i.e The number of most recent frames stacked together, here we use 4 frames, which means each state in Breakout is composed of 4 frames.\n",
    "        num_actions: number of action-value to output, one-to-one correspondence to action in game.\n",
    "\n",
    "        You can add additional arguments as you need.\n",
    "        In the constructor we instantiate modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        ###########################\n",
    "        # YOUR IMPLEMENTATION HERE #\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(in_features=64*7*7, out_features=512)\n",
    "        self.V = nn.Linear(in_features=512, out_features=1)\n",
    "        self.A = nn.Linear(in_features=512, out_features=num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        # YOUR IMPLEMENTATION HERE #\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        V = self.V(x)\n",
    "        A = self.A(x)\n",
    "        Q = V + (A - A.mean(dim=1, keepdim=True))\n",
    "        return Q\n",
    "        ###########################\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "class Agent_DQN(Agent):\n",
    "    def __init__(self, env, args):\n",
    "        \"\"\"\n",
    "        Initialize everything you need here.\n",
    "        For example: \n",
    "            paramters for neural network  \n",
    "            initialize Q net and target Q net\n",
    "            parameters for repaly buffer\n",
    "            parameters for q-learning; decaying epsilon-greedy\n",
    "            ...\n",
    "        \"\"\"\n",
    "        super(Agent_DQN,self).__init__(env)\n",
    "        ###########################\n",
    "        # YOUR IMPLEMENTATION HERE #\n",
    "        \n",
    "        #Gym parameters\n",
    "        self.num_actions = env.action_space.n\n",
    "        \n",
    "        #Buffer\n",
    "        self.buffer_max_len = 20000\n",
    "        self.buffer = deque(maxlen=self.buffer_max_len)\n",
    "        \n",
    "        #Training Parameters\n",
    "        self.num_episodes = 20000\n",
    "        self.batch_size  =32\n",
    "        self.learning_rate = 1.5e-4\n",
    "        self.steps_done = 0\n",
    "        self.target_update = 5000\n",
    "        self.step_start_learning = 5000\n",
    "        self.gamma = 0.999\n",
    "        self.epsilon_start = 1\n",
    "        self.epsilon_end = 0.025\n",
    "        self.epsilon_decay_steps = 100000\n",
    "        self.epsilon = 1\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #TODO: Print this out and see this\n",
    "        self.delta_epsilon = (self.epsilon_start - self.epsilon_end)/self.epsilon_decay_steps\n",
    "           \n",
    "        #Model\n",
    "        self.policy_net = DuelingDQN().to(self.device)\n",
    "        self.target_net = DuelingDQN().to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        #Values to be printed\n",
    "        self.episode_reward_list = []\n",
    "        self.moving_reward_avg = []\n",
    "        \n",
    "        \n",
    "#         if args.test_dqn:\n",
    "#             #you can load your model here\n",
    "#             ###########################\n",
    "#             # YOUR IMPLEMENTATION HERE #\n",
    "    \n",
    "    def init_game_setting(self):\n",
    "        \"\"\"\n",
    "        Testing function will call this function at the begining of new game\n",
    "        Put anything you want to initialize if necessary.\n",
    "        If no parameters need to be initialized, you can leave it as blank.\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        # YOUR IMPLEMENTATION HERE #\n",
    "        \n",
    "        ###########################\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def make_action(self, observation, test=True):\n",
    "        \"\"\"\n",
    "        Return predicted action of your agent\n",
    "        Input:\n",
    "            observation: np.array\n",
    "                stack 4 last preprocessed frames, shape: (84, 84, 4)\n",
    "        Return:\n",
    "            action: int\n",
    "                the predicted action from trained model\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        # YOUR IMPLEMENTATION HERE #\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            observation = torch.tensor(observation, dtype=torch.float, device=self.device).permute(2,0,1).unsqueeze(0)\n",
    "            \n",
    "            #TODO: Change this to test_net?\n",
    "            if test:\n",
    "                return self.target_net(observation).max(1)[1].item()\n",
    "            \n",
    "            result = np.random.uniform()         \n",
    "            \n",
    "            if result > self.epsilon:\n",
    "                return self.policy_net(observation).max(1)[1].item()\n",
    "            else:\n",
    "                return self.env.action_space.sample()     \n",
    "        ###########################\n",
    "        pass\n",
    "        \n",
    "    def push(self, state, reward, action, next_state, done):\n",
    "        \"\"\" You can add additional arguments as you need. \n",
    "        Push new data to buffer and remove the old one if the buffer is full.\n",
    "        \n",
    "        Hints:\n",
    "        -----\n",
    "            you can consider deque(maxlen = 10000) list\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        # YOUR IMPLEMENTATION HERE #\n",
    "        self.buffer.append((state, reward, action, next_state, done))\n",
    "        ###########################\n",
    "        pass\n",
    "        \n",
    "    def replay_buffer(self, batch_size):\n",
    "        \"\"\" You can add additional arguments as you need.\n",
    "        Select batch from buffer.\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        # YOUR IMPLEMENTATION HERE #\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        for sample in batch:\n",
    "            state, reward, action, next_state, done = sample\n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "            actions.append(action)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "        ###########################\n",
    "        return states, rewards, actions, next_states, dones\n",
    "\n",
    "    def update(self):\n",
    "        \n",
    "        if len(self.buffer) < self.step_start_learning:\n",
    "            return     \n",
    "        \n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.delta_epsilon\n",
    "        \n",
    "        states, rewards, actions, next_states, dones = self.replay_buffer(self.batch_size)\n",
    "        loss = self.compute_loss(states, rewards, actions, next_states, dones)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Check this\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp(-1,1)\n",
    "        self.optimizer.step()          \n",
    "\n",
    "    def compute_loss(self, states, rewards, actions, next_states, dones):\n",
    "        non_final_mask = [not done for done in dones]\n",
    "        \n",
    "        states = torch.tensor(states, dtype=torch.float).permute(0,3,1,2).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float).permute(0,3,1,2).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.long).to(self.device)        \n",
    "        \n",
    "        Q_current = self.policy_net.forward(states).gather(1, actions.unsqueeze(1))\n",
    "        Q_current = Q_current.squeeze(1)\n",
    "\n",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "        next_state_values[non_final_mask] = self.target_net(next_states[non_final_mask]).max(1)[0].detach()\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + rewards\n",
    "        \n",
    "        loss = F.smooth_l1_loss(Q_current, expected_state_action_values)\n",
    "        \n",
    "        del states, rewards, actions, next_states, dones, Q_current, next_state_values, expected_state_action_values\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def test(self): \n",
    "        test_env = Environment('BreakoutNoFrameskip-v4', None, atari_wrapper=True, test=True)\n",
    "        rewards = []\n",
    "        seed = 11037\n",
    "        total_episodes=30\n",
    "        test_env.seed(seed)\n",
    "        for i in range(total_episodes):\n",
    "            state = test_env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0.0\n",
    "\n",
    "            #playing one game\n",
    "            while(not done):\n",
    "                action = agent.make_action(state, test=True)\n",
    "                state, reward, done, info = test_env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "            rewards.append(episode_reward)\n",
    "        print('Run %d episodes'%(total_episodes))\n",
    "        print('Mean:', np.mean(rewards))\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Implement your training algorithm here\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        # YOUR IMPLEMENTATION HERE #\n",
    "        for episode in range(self.num_episodes):\n",
    "            \n",
    "            observation = self.env.reset() / 255\n",
    "            episode_steps = 0\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "\n",
    "                action = self.make_action(observation, test=False)\n",
    "                new_observation, reward, done, _ = env.step(action)\n",
    "                new_observation = new_observation / 255\n",
    "                \n",
    "                episode_reward += reward\n",
    "                episode_steps += 1\n",
    "                self.steps_done += 1\n",
    "                \n",
    "                self.push(observation, reward, action, new_observation, done)\n",
    "                \n",
    "                self.update()\n",
    "                \n",
    "                observation = new_observation\n",
    "                \n",
    "                if self.steps_done % self.target_update == 0:\n",
    "                    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            \n",
    "            self.episode_reward_list.append(episode_reward)\n",
    "            self.moving_reward_avg.append(np.average(np.array(self.episode_reward_list[-30:])))\n",
    "\n",
    "            if episode % 100 == 0:\n",
    "                print('episode: {} average reward: {} episode length: {}'.format(episode,\n",
    "                                                                        self.moving_reward_avg[-1],\n",
    "                                                                        episode_steps))\n",
    "                torch.save(self.policy_net.state_dict(), 'test_model.pt')\n",
    "            \n",
    "            if episode % 500 == 0:\n",
    "                self.test()\n",
    "        self.moving_reward_avg = np.array(self.moving_reward_avg)\n",
    "        np.savetxt(\"rewards.csv\", self.moving_reward_avg, delimiter=\",\")            \n",
    "        print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "episode: 0 average reward: 2.0 episode length: 121\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "Run 30 episodes\n",
      "Mean: 0.3\n",
      "episode: 25 average reward: 0.3076923076923077 episode length: 23\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "Run 30 episodes\n",
      "Mean: 0.3\n",
      "episode: 50 average reward: 0.4666666666666667 episode length: 23\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "Run 30 episodes\n",
      "Mean: 0.3\n",
      "episode: 75 average reward: 0.3 episode length: 23\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "Run 30 episodes\n",
      "Mean: 0.3\n",
      "episode: 100 average reward: 0.1 episode length: 23\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "Run 30 episodes\n",
      "Mean: 0.3\n",
      "episode: 125 average reward: 0.06666666666666667 episode length: 70\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "Run 30 episodes\n",
      "Mean: 0.3\n",
      "episode: 150 average reward: 0.2 episode length: 51\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-818a571879dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matari_wrapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent_DQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-216b3023f430>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m25\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-216b3023f430>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mtest_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mepisode_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/BreakoutRL/environment.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mcurrent\u001b[0m \u001b[0mRGB\u001b[0m \u001b[0mscreen\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m210\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         '''\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/BreakoutRL/atari_wrapper.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/BreakoutRL/atari_wrapper.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/BreakoutRL/atari_wrapper.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwas_real_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# check current lives, make loss of life terminal,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/BreakoutRL/atari_wrapper.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# Note that the observation on the done=True frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# doesn't matter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mmax_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     37\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     38\u001b[0m           initial=_NoValue, where=True):\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env_name = 'BreakoutNoFrameskip-v4'\n",
    "env = Environment(env_name, None, atari_wrapper=True)\n",
    "agent = Agent_DQN(env, None)\n",
    "agent.train()\n",
    "torch.save(agent.target_net, 'test_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1e-06"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
