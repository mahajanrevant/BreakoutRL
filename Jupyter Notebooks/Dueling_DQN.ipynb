{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dueling DQN",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "With Colab Pro you have priority access to our fastest GPUs. For example, you may get a T4 or P100 GPU at times when most users of standard Colab receive a slower K80 GPU. You can see what GPU you've been assigned at any time by executing the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23TOba33L4qf",
        "outputId": "e75df318-50a4-47b8-cde6-741145d8de9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Nov 13 22:57:37 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the Runtime > Change runtime type menu, and then set the hardware accelerator dropdown to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "With Colab Pro you have the option to access high-memory VMs when they are available. To set your notebook preference to use a high-memory runtime, select the Runtime > 'Change runtime type' menu, and then select High-RAM in the Runtime shape dropdown.\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1G82GuO-tez",
        "outputId": "dbbcaf6f-d2fe-4c17-d206-1ad0e65a1fc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.4 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkHP5B9IxcgL",
        "outputId": "cbd0eac1-0299-41c0-9d7a-e8c3af7acd16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install opencv-python-headless gym==0.10.4 gym[atari]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opencv-python-headless\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/e9/57d869561389884136be65a2d1bc038fe50171e2ba348fda269a4aab8032/opencv_python_headless-4.4.0.46-cp36-cp36m-manylinux2014_x86_64.whl (36.7MB)\n",
            "\u001b[K     |████████████████████████████████| 36.7MB 84kB/s \n",
            "\u001b[?25hCollecting gym==0.10.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/e5/4dae1de6534221f74895c8a95ae4eedc816a5fa003db1d4d608cbdc28b35/gym-0.10.4.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 58.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python-headless) (1.18.5)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.4) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym==0.10.4) (1.15.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.4) (1.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.4) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.4) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.4) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.4) (2020.6.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym==0.10.4) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.10.4-cp36-none-any.whl size=1581557 sha256=e42f9eb96ee1ef45f02ce29a6dbd82c32790a651e03adaa1971975a03adabc2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/41/49/1581815cc493e09e494ba013c2f6f29108b8e2adf40db4b21d\n",
            "Successfully built gym\n",
            "\u001b[31mERROR: dopamine-rl 1.0.5 has requirement gym>=0.10.5, but you'll have gym 0.10.4 which is incompatible.\u001b[0m\n",
            "Installing collected packages: opencv-python-headless, gym\n",
            "  Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.10.4 opencv-python-headless-4.4.0.46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0gdO23Mxc7Z"
      },
      "source": [
        "from environment import Environment\n",
        "from agent import Agent\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import deque\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from agent import Agent\n",
        "#from dqn_model import DQN\n",
        "\n",
        "torch.manual_seed(595)\n",
        "np.random.seed(595)\n",
        "random.seed(595)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF2cx6BGywUn"
      },
      "source": [
        "# There are 3 convolutional layers followed by\n",
        "# 2 fully-connected layers. The first convolutional layer has\n",
        "# 32 8×8 filters with stride 4, the second 64 4×4 filters with\n",
        "# stride 2, and the third and final convolutional layer consists\n",
        "# 64 3 × 3 filters with stride 1. As shown in Figure 1, the\n",
        "# dueling network splits into two streams of fully connected\n",
        "# layers. The value and advantage streams both have a fullyconnected layer with 512 units. The final hidden layers of\n",
        "# the value and advantage streams are both fully-connected\n",
        "# with the value stream having one output and the advantage\n",
        "# as many outputs as there are valid actions2\n",
        "# . We combine the\n",
        "# value and advantage streams using the module described by\n",
        "# Equation (9). Rectifier non-linearities (Fukushima, 1980)\n",
        "# are inserted between all adjacent layers.\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DuelingDQN(nn.Module):\n",
        "    \"\"\"Initialize a deep Q-learning network\n",
        "\n",
        "    Hints:\n",
        "    -----\n",
        "        Original paper for DQN\n",
        "    https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
        "\n",
        "    This is just a hint. You can build your own structure.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels=4, num_actions=4):\n",
        "        \n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        -----------\n",
        "        in_channels: number of channel of input.\n",
        "                i.e The number of most recent frames stacked together, here we use 4 frames, which means each state in Breakout is composed of 4 frames.\n",
        "        num_actions: number of action-value to output, one-to-one correspondence to action in game.\n",
        "\n",
        "        You can add additional arguments as you need.\n",
        "        In the constructor we instantiate modules and assign them as\n",
        "        member variables.\n",
        "        \"\"\"\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        ###########################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "        self.fc1 = nn.Linear(in_features=64*7*7, out_features=512)\n",
        "        self.V = nn.Linear(in_features=512, out_features=1)\n",
        "        self.A = nn.Linear(in_features=512, out_features=num_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Tensor of input data and we must return\n",
        "        a Tensor of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Tensors.\n",
        "        \"\"\"\n",
        "        ###########################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        #x = x.view(x.size(0), -1)\n",
        "        x = x.contiguous().view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        V = self.V(x)\n",
        "        A = self.A(x)\n",
        "        Q = V + (A - A.mean(dim=1, keepdim=True))\n",
        "        return Q\n",
        "        ###########################\n",
        "        return x\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVAK37l82hy1",
        "outputId": "e61662e7-e7eb-4939-95a8-2a6236a76af1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "t = torch.tensor([[[1, 2],\n",
        "                       [3, 4]],\n",
        "                      [[5, 6],\n",
        "                       [7, 8]]])\n",
        "t = t.view(-1)\n",
        "print(t)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oXOtCDny0Oe"
      },
      "source": [
        "class Agent_DQN(Agent):\n",
        "    def __init__(self, env, args):\n",
        "        \"\"\"\n",
        "        Initialize everything you need here.\n",
        "        For example: \n",
        "            paramters for neural network  \n",
        "            initialize Q net and target Q net\n",
        "            parameters for repaly buffer\n",
        "            parameters for q-learning; decaying epsilon-greedy\n",
        "            ...\n",
        "        \"\"\"\n",
        "        super(Agent_DQN,self).__init__(env)\n",
        "        ###########################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        \n",
        "        #Gym parameters\n",
        "        self.num_actions = env.action_space.n\n",
        "        \n",
        "        #Buffer\n",
        "        self.buffer_max_len = 20000\n",
        "        self.buffer = deque(maxlen=self.buffer_max_len)\n",
        "        \n",
        "        #Training Parameters\n",
        "        self.num_episodes = 20000\n",
        "        self.batch_size  =32\n",
        "        self.learning_rate = 1.5e-4\n",
        "        self.steps_done = 0\n",
        "        self.target_update = 5000\n",
        "        self.step_start_learning = 5000\n",
        "        self.gamma = 0.999\n",
        "        self.epsilon_start = 1\n",
        "        self.epsilon_end = 0.025\n",
        "        self.epsilon_decay_steps = 100000\n",
        "        self.epsilon = 1\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(self.device)\n",
        "        #TODO: Print this out and see this\n",
        "        self.delta_epsilon = (self.epsilon_start - self.epsilon_end)/self.epsilon_decay_steps\n",
        "           \n",
        "        #Model\n",
        "        self.policy_net = DuelingDQN().to(self.device)\n",
        "        self.target_net = DuelingDQN().to(self.device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
        "        \n",
        "        #Values to be printed\n",
        "        self.episode_reward_list = []\n",
        "        self.moving_reward_avg = []\n",
        "        \n",
        "        \n",
        "#         if args.test_dqn:\n",
        "#             #you can load your model here\n",
        "#             ###########################\n",
        "#             # YOUR IMPLEMENTATION HERE #\n",
        "    \n",
        "    def init_game_setting(self):\n",
        "        \"\"\"\n",
        "        Testing function will call this function at the begining of new game\n",
        "        Put anything you want to initialize if necessary.\n",
        "        If no parameters need to be initialized, you can leave it as blank.\n",
        "        \"\"\"\n",
        "        ###########################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        \n",
        "        ###########################\n",
        "        pass\n",
        "    \n",
        "    \n",
        "    def make_action(self, observation, test=True):\n",
        "        \"\"\"\n",
        "        Return predicted action of your agent\n",
        "        Input:\n",
        "            observation: np.array\n",
        "                stack 4 last preprocessed frames, shape: (84, 84, 4)\n",
        "        Return:\n",
        "            action: int\n",
        "                the predicted action from trained model\n",
        "        \"\"\"\n",
        "        ###########################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            observation = torch.tensor(observation, dtype=torch.float, device=self.device).permute(2,0,1).unsqueeze(0)\n",
        "            \n",
        "            #TODO: Change this to test_net?\n",
        "            if test:\n",
        "                return self.target_net(observation).max(1)[1].item()\n",
        "            \n",
        "            result = np.random.uniform()         \n",
        "            \n",
        "            if result > self.epsilon:\n",
        "                return self.policy_net(observation).max(1)[1].item()\n",
        "            else:\n",
        "                return self.env.action_space.sample()     \n",
        "        ###########################\n",
        "        pass\n",
        "        \n",
        "    def push(self, state, reward, action, next_state, done):\n",
        "        \"\"\" You can add additional arguments as you need. \n",
        "        Push new data to buffer and remove the old one if the buffer is full.\n",
        "        \n",
        "        Hints:\n",
        "        -----\n",
        "            you can consider deque(maxlen = 10000) list\n",
        "        \"\"\"\n",
        "        ###########################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        self.buffer.append((state, reward, action, next_state, done))\n",
        "        ###########################\n",
        "        pass\n",
        "        \n",
        "    def replay_buffer(self, batch_size):\n",
        "        \"\"\" You can add additional arguments as you need.\n",
        "        Select batch from buffer.\n",
        "        \"\"\"\n",
        "        ###########################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states = []\n",
        "        rewards = []\n",
        "        actions = []\n",
        "        next_states = []\n",
        "        dones = []\n",
        "        for sample in batch:\n",
        "            state, reward, action, next_state, done = sample\n",
        "            states.append(state)\n",
        "            rewards.append(reward)\n",
        "            actions.append(action)\n",
        "            next_states.append(next_state)\n",
        "            dones.append(done)\n",
        "        ###########################\n",
        "        return states, rewards, actions, next_states, dones\n",
        "\n",
        "    def update(self):\n",
        "        \n",
        "        if len(self.buffer) < self.step_start_learning:\n",
        "            return     \n",
        "        \n",
        "        if self.epsilon > self.epsilon_end:\n",
        "            self.epsilon -= self.delta_epsilon\n",
        "        \n",
        "        states, rewards, actions, next_states, dones = self.replay_buffer(self.batch_size)\n",
        "        loss = self.compute_loss(states, rewards, actions, next_states, dones)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # Check this\n",
        "        for param in self.policy_net.parameters():\n",
        "            param.grad.data.clamp(-1,1)\n",
        "        self.optimizer.step()          \n",
        "\n",
        "    def compute_loss(self, states, rewards, actions, next_states, dones):\n",
        "        non_final_mask = [not done for done in dones]\n",
        "        \n",
        "        states = torch.tensor(states, dtype=torch.float).permute(0,3,1,2).to(self.device)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float).to(self.device)\n",
        "        actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float).permute(0,3,1,2).to(self.device)\n",
        "        dones = torch.tensor(dones, dtype=torch.long).to(self.device)        \n",
        "        \n",
        "        Q_current = self.policy_net.forward(states).gather(1, actions.unsqueeze(1))\n",
        "        Q_current = Q_current.squeeze(1)\n",
        "\n",
        "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
        "        next_state_values[non_final_mask] = self.target_net(next_states[non_final_mask]).max(1)[0].detach()\n",
        "        expected_state_action_values = (next_state_values * self.gamma) + rewards\n",
        "        \n",
        "        loss = F.smooth_l1_loss(Q_current, expected_state_action_values)\n",
        "        \n",
        "        del states, rewards, actions, next_states, dones, Q_current, next_state_values, expected_state_action_values\n",
        "        \n",
        "        return loss\n",
        "        \n",
        "    def test(self): \n",
        "        test_env = Environment('BreakoutNoFrameskip-v4', None, atari_wrapper=True, test=True)\n",
        "        rewards = []\n",
        "        seed = 11037\n",
        "        total_episodes=30\n",
        "        test_env.seed(seed)\n",
        "        for i in range(total_episodes):\n",
        "            state = test_env.reset()\n",
        "            done = False\n",
        "            episode_reward = 0.0\n",
        "\n",
        "            #playing one game\n",
        "            while(not done):\n",
        "                action = agent.make_action(state, test=True)\n",
        "                state, reward, done, info = test_env.step(action)\n",
        "                episode_reward += reward\n",
        "\n",
        "            rewards.append(episode_reward)\n",
        "        print('Run %d episodes'%(total_episodes))\n",
        "        print('Mean:', np.mean(rewards))\n",
        "    \n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Implement your training algorithm here\n",
        "        \"\"\"\n",
        "        ###########################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        for episode in range(self.num_episodes):\n",
        "            \n",
        "            observation = self.env.reset() / 255\n",
        "            episode_steps = 0\n",
        "            episode_reward = 0\n",
        "            done = False\n",
        "            \n",
        "            while not done:\n",
        "\n",
        "                action = self.make_action(observation, test=False)\n",
        "                new_observation, reward, done, _ = env.step(action)\n",
        "                new_observation = new_observation / 255\n",
        "                \n",
        "                episode_reward += reward\n",
        "                episode_steps += 1\n",
        "                self.steps_done += 1\n",
        "                \n",
        "                self.push(observation, reward, action, new_observation, done)\n",
        "                \n",
        "                self.update()\n",
        "                \n",
        "                observation = new_observation\n",
        "                \n",
        "                if self.steps_done % self.target_update == 0:\n",
        "                    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "            \n",
        "            self.episode_reward_list.append(episode_reward)\n",
        "            self.moving_reward_avg.append(np.average(np.array(self.episode_reward_list[-30:])))\n",
        "\n",
        "            if episode % 100 == 0:\n",
        "                print('episode: {} average reward: {} episode length: {}'.format(episode,\n",
        "                                                                        self.moving_reward_avg[-1],\n",
        "                                                                        episode_steps))\n",
        "                torch.save(self.policy_net.state_dict(), 'test_model.pt')\n",
        "            \n",
        "            if episode % 500 == 0:\n",
        "                self.test()\n",
        "          \n",
        "        self.moving_reward_avg = np.array(self.moving_reward_avg)\n",
        "        np.savetxt(\"rewards.csv\", self.moving_reward_avg, delimiter=\",\")            \n",
        "        print(\"Done\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF4NEIS3y5Jr",
        "outputId": "402c622e-9076-4c48-e8da-7d1db246ac70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env_name = 'BreakoutNoFrameskip-v4'\n",
        "env = Environment(env_name, None, atari_wrapper=True)\n",
        "agent = Agent_DQN(env, None)\n",
        "agent.train()\n",
        "torch.save(agent.policy_net, 'test_model.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
            "cuda:0\n",
            "episode: 0 average reward: 1.0 episode length: 69\n",
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
            "Run 30 episodes\n",
            "Mean: 0.16666666666666666\n",
            "episode: 100 average reward: 0.36666666666666664 episode length: 70\n",
            "episode: 200 average reward: 0.4666666666666667 episode length: 23\n",
            "episode: 300 average reward: 0.1 episode length: 69\n",
            "episode: 400 average reward: 0.2 episode length: 23\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}